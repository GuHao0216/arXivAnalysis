{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cs21_paperUrl.json') as f:\n",
    "    paperUrlList = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initTemp():\n",
    "    return {\n",
    "        \"id\": None,\n",
    "        \"submitter\": None,\n",
    "        \"authors\": None,\n",
    "        \"title\": None,\n",
    "        \"comments\": None,\n",
    "        \"journal-ref\": None,\n",
    "        \"doi\": None,\n",
    "        \"report-no\": None,\n",
    "        \"categories\": None,\n",
    "        \"license\": None,\n",
    "        \"abstract\": None,\n",
    "        \"versions\": None,\n",
    "        \"update_date\": None,\n",
    "        \"authors_parsed\": None\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime\n",
    "def setInfo(temp,soup):\n",
    "    # submitter\n",
    "    submitter_temp = soup.find('div',attrs={'class':'submission-history'}).get_text()\n",
    "    temp['submitter']=submitter_temp[submitter_temp.find(':')+1:submitter_temp.find('[')].strip()\n",
    "    # authors\n",
    "    auth_temp = soup.find('div',attrs={'class':'authors'}).get_text()\n",
    "    temp['authors'] = auth_temp[auth_temp.find(':')+1:]\n",
    "    # title\n",
    "    temp['title'] = soup.find('meta',attrs={'name':'citation_title'}).get('content')\n",
    "    # comments\n",
    "    temp['comments'] = None if soup.find('td',attrs={'class':'comments'})==None else soup.find('td',attrs={'class':'comments'}).get_text()\n",
    "    # doi\n",
    "    temp['doi']=None if soup.find('meta',attrs={'name':'citation_doi'})==None else soup.find('meta',attrs={'name':'citation_doi'}).get('content')\n",
    "    # journal-ref and report-no\n",
    "    tempList = [i.find_parent() for i in soup.find_all('td',attrs={'class':'jref'})]\n",
    "    for i in tempList:\n",
    "        label = i.find('td').get_text()\n",
    "        if label=='Journal\\xa0reference:':\n",
    "            temp['journal-ref'] = i.find('td',attrs={'class':'jref'}).get_text()\n",
    "        if label=='Report\\xa0number:':\n",
    "            temp['report-no'] = i.find('td',attrs={'class':'jref'}).get_text()\n",
    "    # categories\n",
    "    sub_temp = soup.find('td',attrs={'class':'subjects'}).get_text()\n",
    "    p1 = re.compile(r'[(](.*?)[)]', re.S)\n",
    "    temp['categories'] = ' '.join(re.findall(p1, sub_temp))\n",
    "    # license\n",
    "    license_temp = soup.find('div',attrs={'class':'abs-license'}).find('a').get('href')\n",
    "    temp['license'] = None if license_temp=='http://arxiv.org/licenses/assumed-1991-2003/' else license_temp\n",
    "    # abstract\n",
    "    temp['abstract'] = soup.find('meta',attrs={'name':'citation_abstract'}).get('content')\n",
    "    # versions\n",
    "    versions_temp = soup.find('div',attrs={'class':'submission-history'}).get_text()\n",
    "    versions_row_list = versions_temp.split('[v')\n",
    "    version_list = []\n",
    "    for i in range(2,len(versions_row_list)):\n",
    "        version_cell = {}\n",
    "        version_cell['version'] = 'v'+str(i-1)\n",
    "        strList = versions_row_list[i].split('\\n')\n",
    "        for i in strList[::-1]:\n",
    "            if(i.strip()!=''):\n",
    "                i_list = i.strip().replace('UTC','GMT').split(' ')\n",
    "                version_cell['created'] = ' '.join(i_list[:len(i_list)-2])\n",
    "                break\n",
    "        version_list.append(version_cell)\n",
    "    temp['versions'] = version_list\n",
    "    # update_date\n",
    "    temp['update_date'] = datetime.datetime.now().date().strftime(\"%Y-%m-%d\")\n",
    "    # authors_parsed\n",
    "    authorsParsedList = []\n",
    "    authorsParsedSoup = soup.find_all('meta',attrs={'name':'citation_author'})\n",
    "    for _ in authorsParsedSoup:\n",
    "        auth_temp = _.get('content').split(', ')\n",
    "        auth_temp.append('')\n",
    "        authorsParsedList.append(auth_temp)\n",
    "    temp['authors_parsed'] = authorsParsedList\n",
    "    return temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "def getPaperInfo(tname,paperUrlList,filename):\n",
    "    url_error_list = []\n",
    "    msg_error_list = []\n",
    "    paperInfo_list = []\n",
    "    for i,_ in enumerate(paperUrlList):\n",
    "        if i%50==0:\n",
    "            print(tname+':'+str(i))\n",
    "        attempts = 0\n",
    "        success = False\n",
    "        while attempts < 3 and not success:\n",
    "            try:\n",
    "                url_soup = BeautifulSoup(requests.get(_).text)\n",
    "                success = True\n",
    "            except:\n",
    "                attempts += 1\n",
    "                if attempts == 3:\n",
    "                    print('error:'+ _ )\n",
    "                    url_error_list.append(_)\n",
    "                break\n",
    "        if success:\n",
    "            temp = initTemp()\n",
    "            temp['id']=_.split('/')[-1]\n",
    "            try:\n",
    "                temp = setInfo(temp,url_soup)\n",
    "                paperInfo_list.append(temp)\n",
    "            except Exception as e:\n",
    "                msg_error_list.append(temp)\n",
    "                traceback.print_exc()\n",
    "            \n",
    "    with open(filename,'a') as f:\n",
    "        for _ in paperInfo_list:\n",
    "            f.write(json.dumps(_)+'\\n')\n",
    "    with open(tname,'w') as f:\n",
    "        f.write(json.dumps(url_error_list))\n",
    "    with open(tname+'Msgerr','w') as f:\n",
    "        f.write(json.dumps(msg_error_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77510"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paperUrlList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "task = []\n",
    "\n",
    "for i,j in zip(range(80000//5000),range(0,80000,5000)):\n",
    "    if j+5000<77510:\n",
    "        task.append(threading.Thread(target=getPaperInfo,args=('T{}'.format(i),paperUrlList[j:j+5000],'paperInfo{}.json'.format(i))))\n",
    "    else:\n",
    "        task.append(threading.Thread(target=getPaperInfo,args=('T{}'.format(i),paperUrlList[j:],'paperInfo{}.json'.format(i))))\n",
    "\n",
    "for _ in task:\n",
    "    _.setDaemon(True)\n",
    "    _.start()\n",
    "    \n",
    "for _ in task:\n",
    "    _.join()\n",
    "\n",
    "print('end')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e6824cc290bf2c970f73ddb844979daf1344d8c7e466dca491c3c11f7a27722"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
